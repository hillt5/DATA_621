---
title: "Data 621 - HW4"
author: "Devin Teran, Atina Karim, Tom Hill, Amit Kapoor"
date: "5/2/2021"
output:
  pdf_document:
    toc: TRUE
    toc_depth: 2 
  html_document:
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```

```{r libraries, include=FALSE}


# Libraries


library(stringr)
library(tidyr)
library(DataExplorer)        
library(dplyr)
library(visdat)
library(pROC)
library(mice)
library(corrplot)
library(MASS)
library(caret)
library(e1071)
library(rbin)

library(GGally)
library(ggplot2)
library(readr)
library(reshape2)
library(purrr)

set.seed(2012)


```


```{r data}
insurance <- read.csv('https://raw.githubusercontent.com/hillt5/DATA_621/master/HW4/insurance_training_data.csv', stringsAsFactors =  FALSE)
```


## Overview


In this homework assignment, you will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The
first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.  

Your objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:


#### Response Variables:

TARGET_FLAG Was Car in a crash? 1=YES 0=NO None  
TARGET_AMT If car was in a crash, what was the cost  

#### Explanatory Variables:

AGE Age of Driver 
BLUEBOOK Value of Vehicle
CAR_AGE Vehicle Age 
CAR_TYPE Type of Car
CAR_USE Vehicle Use
CLM_FREQ # Claims (Past 5 Years)
EDUCATION Max Education Level
HOMEKIDS # Children at Home 
HOME_VAL Home Value 
INCOME Income
JOB Job Category
KIDSDRIV Number of Driving Children 
MSTATUS Marital Status 
MVR_PTS Motor Vehicle Record Points 
OLDCLAIM Total Claims (Past 5 Years)  
PARENT1 Single Parent  
RED_CAR A Red Car  
REVOKED License Revoked (Past 7 Years)  
SEX Gender  
TIF Time in Force 
TRAVTIME Distance to Work 
URBANICITY Home/Work Area 
YOJ Years on Job  


```{r, before-fix}

head(insurance)
summary(insurance)

insurance_fix <- dplyr::select(insurance, -INDEX)

```

There are several recurring isues with some columns: all columns containing money amounts have incomptaible punctuation and characters. Also, categorical variables neeed to be changed to factors and their factor names edited for intelligibility.  



```{r, numeric-vars}

insurance_fix$HOME_VAL <- substr(insurance_fix$HOME_VAL, 2, nchar(insurance_fix$HOME_VAL)) # remove the dollar sign 
insurance_fix$HOME_VAL <- as.numeric(str_remove_all(insurance_fix$HOME_VAL, "[[:punct:]]")) # remove the comma and periods for money

insurance_fix$BLUEBOOK<- substr(insurance_fix$BLUEBOOK , 2, nchar(insurance_fix$BLUEBOOK ))
insurance_fix$BLUEBOOK<- as.numeric(str_remove_all(insurance_fix$BLUEBOOK,"[[:punct:]]"))

insurance_fix$INCOME  <- substr(insurance_fix$INCOME, 2, nchar(insurance_fix$INCOME))
insurance_fix$INCOME <- as.numeric(str_remove_all(insurance_fix$INCOME, "[[:punct:]]"))

insurance_fix$OLDCLAIM <- substr(insurance_fix$OLDCLAIM, 2, nchar(insurance_fix$OLDCLAIM))
insurance_fix$OLDCLAIM <- as.numeric(str_remove_all(insurance_fix$OLDCLAIM, "[[:punct:]]"))

```


```{r, categorical-vars}

insurance_fix$MSTATUS = as.factor(str_remove(insurance_fix$MSTATUS, 'z_')) #several variables have a a recurring typo
insurance_fix$PARENT1 = as.factor(str_remove(insurance_fix$PARENT1, 'z_'))
insurance_fix$EDUCATION = str_replace(insurance_fix$EDUCATION, '<', 'Less than ') #change < to less than symbol to avoid confusion
insurance_fix$SEX= as.factor(str_remove(insurance_fix$SEX, 'z_'))
insurance_fix$EDUCATION = as.factor(str_remove(insurance_fix$EDUCATION, 'z_'))
insurance_fix$JOB[insurance_fix$JOB == ""] <- 'Other Job' #recode blank spaces as 'Other Job'
insurance_fix$JOB = as.factor(str_remove(insurance_fix$JOB, 'z_'))
insurance_fix$CAR_USE = as.factor(str_remove(insurance_fix$CAR_USE, 'z_'))
insurance_fix$CAR_TYPE = as.factor(str_remove(insurance_fix$CAR_TYPE, 'z_'))
insurance_fix$URBANICITY = as.factor(str_remove(insurance_fix$URBANICITY, 'z_'))
insurance_fix$REVOKED = as.factor(str_remove(insurance_fix$REVOKED, 'z_'))
insurance_fix$RED_CAR = as.factor(str_remove(insurance_fix$RED_CAR, 'z_'))
```


```{r, after-fix}

summary(insurance_fix)

```


The fixed dataframe now only includes columns that are numeric or factors.  Car age appears to have some values less than 1, including a negative values. These will be chnaged to the mode of 1.


```{r, car-age}

insurance_fix$CAR_AGE[insurance_fix$CAR_AGE <1] <- 1



```

## Data Exploration

### Categorical variables

```{r, levels}

for (i in 4:ncol(insurance_fix)) {
  if (class((insurance_fix[,i])) == 'factor') {
      print(names(insurance_fix[i]))
      print(levels(insurance_fix[,i]))
  }

}


```

Looking at categorical variables, most of the columns are binary.


### Numeric Variables

```{r, histograms}


plot_histogram(insurance_fix)


```

Many numeric variables feature the value of zero as a mode.


### Missing Values



```{r, missing-values}

round(colSums(is.na(insurance_fix))/nrow(insurance_fix),3)

vis_dat(insurance_fix %>% dplyr:: select(YOJ, INCOME, HOME_VAL, CAR_AGE))


```

Four variables have missing values, however there doesn't appear to be a pattern and it's safe to assume they're missing at random.


## Build Models

### Binary Logistic Regression


```{r, first-model}


insurance_logistic_model <- glm(insurance_fix, family = 'binomial', formula = TARGET_FLAG~.-TARGET_AMT)

summary(insurance_logistic_model)

```

The first model to consider includes all given variables and does not impute any values.


```{r, performance-function}
get_cv_performance <- function(data_frame, model, split = 0.8) {  ### input is dataframe for partitioning, model as generated by 'glm' function, by default 5-fold cross-validation
  n <- ncol(data_frame) #number of columns in original dataframe
  train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
  trainIndex <- createDataPartition(data_frame[,n], p=split, list=FALSE)
  data_train <- data_frame[trainIndex,]
  data_test <- data_frame[-trainIndex,]
  
  x_test <- data_test[,2:n] #explanatory variables
  y_test <- data_test[,1]  #response variable
  predictions <- predict(model, x_test, type = 'response')
  
  return(confusionMatrix(data = (as.factor(as.numeric(predictions>0.5))), reference = as.factor(y_test)))
  
  return(plot(roc(y_test, predictions),print.auc=TRUE))
  
}
```


```{r, roc-function}
get_roc <- function(data_frame, model, split = 0.8) {  ### input is dataframe for partitioning, model as generated by 'glm' function
  n <- ncol(data_frame) #number of columns in original dataframe
  train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
  trainIndex <- createDataPartition(data_frame[,n], p=split, list=FALSE)
  data_train <- data_frame[trainIndex,]
  data_test <- data_frame[-trainIndex,]
  
  x_test <- data_test[,2:n] #explanatory variables
  y_test <- data_test[,1]  #response variable
  predictions <- predict(model, x_test, type = 'response')
  return(plot(roc(y_test, predictions),print.auc=TRUE))
  
}
```




```{r, logistic-model-performance}

get_cv_performance(insurance_fix, insurance_logistic_model)
get_roc(insurance_fix, insurance_logistic_model)


```



```{r, imputed-model}

insurance_impute <- mice(insurance_fix, method = 'cart', m = 1)

imputed_lm <- glm.mids(data = insurance_impute, formula = TARGET_FLAG ~.-TARGET_AMT, family = 'binomial')


imputed_lm

```


The second model imputes values using the 'mice' library using classification and regression trees.


```{r, imputed-model-performance}

get_cv_performance(insurance_fix, imputed_lm$analyses[[1]])
get_roc(insurance_fix, imputed_lm$analyses[[1]])


```


### Addressing Zeroes using Binning



The histograms for several variables indicate that there many with an overrepresentation of 'zero' values. Some of the worst offenders include CAR_AGE, HOME_VAL, HOMEKIDS, KIDSDRIV, OLDCLAIM, TIF, and YOJ.  INCOME also has many 'zero' or very low values, and also similar to CAR_AGE and HOME_VAL because, omitting zero, the rest of the distributions appear to be skewed, approximately normal distributions. To avoid problems with interpretation, the next model will consider these continuous variables as categorical variables defined as a number range.

```{r, binned-data}


insurance_bins <- insurance_fix %>%
  mutate(CAR_AGE_BIN=cut(CAR_AGE, breaks=c(-Inf, 1, 3, 12, Inf), labels=c("New","Like New","Average", 'Old'))) %>% #four level fator for car age
  mutate(HOME_VAL_BIN=cut(HOME_VAL, breaks=c(-Inf, 0, 50000, 150000, 250000, Inf), labels=c("Zero", "$0-$50k", "$50k-$150k","$150k-$250k", 'Over $250k'))) %>% #bins for zero, plus four other price ranges
  mutate(HAS_HOME_KIDS = as.factor(case_when(HOMEKIDS == 0 ~ 'No kids', HOMEKIDS > 0 ~ ('Has kids')))) %>% #binary variable for whether family has kids
  mutate(HAS_KIDSDRIV = as.factor(case_when(KIDSDRIV == 0 ~ 'No kids driving', KIDSDRIV > 0 ~ 'Has kids driving'))) %>% #binary variable for whether family has kids driving
  mutate(OLDCLAIM_BIN =cut(OLDCLAIM, breaks=c(-Inf, 0, 3000, 6000, 9000, Inf), labels=c("Zero","$0-$3k", "$3k-$6k", "$6k-$9k",'Over $9k'))) %>% #bins for zero, plus four other price ranges based on quartiles
  mutate(TIF_BIN =cut(TIF, breaks=c(-Inf, 0, 1, 4, 7, Inf), labels=c("Zero","Less than 1 year", "1-4 years", "4-7 years",'Over 7 years'))) %>% #bins for zero, plus four other price ranges based on quartiles
  mutate(YOJ_BIN =cut(YOJ, breaks=c(-Inf, 0, 10, 15, Inf), labels=c("Zero","Less than 10 years", 'Between 10-15 years', 'Over 15 years'))) %>% #bins for zero, plus three other categories based on quartiles
  dplyr::select(-c(CAR_AGE, HOME_VAL, HOMEKIDS, KIDSDRIV, OLDCLAIM, TIF, YOJ)) #drop the binned features



summary(insurance_bins)
head(insurance_bins)

```

```{r, binned-model}


binned_lm <- glm(data = insurance_bins, formula = TARGET_FLAG ~.-TARGET_AMT, family = 'binomial')


summary(binned_lm)

```

This and the consequent model considers all binned variables plus old variables. The next model provides a combination of imputation and binning

```{r, binned-model-performance}

get_cv_performance(insurance_bins, binned_lm)
get_roc(insurance_bins, binned_lm)


```

```{r, binned-and-imputed-model}

insurance_binned_impute <- mice(insurance_bins, method = 'cart', m = 1)

binned_imputed_lm <- glm.mids(data = insurance_binned_impute, formula = TARGET_FLAG ~.-TARGET_AMT, family = 'binomial')


binned_imputed_lm

```



```{r, binned-and-imputed-model-performance}

get_cv_performance(insurance_bins, binned_imputed_lm$analyses[[1]])
get_roc(insurance_bins, binned_imputed_lm$analyses[[1]])

```



### Multiple Linear Regression

```{r, multiple-linear-regression}

### Below code shows output for preliminary regression modelling insurance payout given that a claim has been predicted. R-squared values are very low, but this assumes that a correct prediction from the binary logistic model has been made

#summary(lm(insurance_bins[insurance_bins$TARGET_AMT > 0, ], formula = TARGET_AMT~.-TARGET_FLAG))  



```






## Model Selection








